"""
Tool Executor for Agentic AI System

This module executes tools based on the planner's decision.
It does not decide which tool to use or generate final answers.
"""
from agent.planner import call_llm
from typing import Dict, Any
from tools import pinecone_tool, neo4j_tool, web_search_tool
import json


def _generate_answer_from_tool_output(user_question: str, tool_output: Any, tool_name: str) -> str:
    """
    Convert raw tool output into a human-readable answer using LLM.
    
    Args:
        user_question: The original user question
        tool_output: Raw results from the tool
        tool_name: Name of the tool that produced the output
        
    Returns:
        str: Human-readable answer
    """
    # Build context from tool output
    if tool_name == "vector_search":
        # Extract text chunks from vector search results
        context_chunks = []
        if isinstance(tool_output, list):
            for item in tool_output:
                if isinstance(item, dict) and "text" in item:
                    context_chunks.append(item["text"])
        
        context = "\n\n".join(context_chunks[:3])  # Use top 3 results
        
        system_prompt = "You are a helpful AI assistant. Answer the question using ONLY the provided context. If the answer is not in the context, say you don't know."
        user_prompt = f"Context:\n{context}\n\nQuestion: {user_question}\n\nAnswer:"
    
    elif tool_name == "graph_search":
        # Extract names/relationships from graph results
        formatted_results = []
        if isinstance(tool_output, list):
            for item in tool_output:
                if isinstance(item, dict) and "name" in item:
                    role = item.get("role", "")
                    if role:
                        formatted_results.append(f"{item['name']} - {role}")
                    else:
                        formatted_results.append(item["name"])
        
        context = "\n".join(formatted_results) if formatted_results else json.dumps(tool_output)
        
        system_prompt = "You are a helpful AI assistant. Answer the question based ONLY on the provided graph data. Be concise and specific."
        user_prompt = f"Graph data:\n{context}\n\nQuestion: {user_question}\n\nAnswer:"
    
    elif tool_name == "web_search":
        # Extract web search results
        if isinstance(tool_output, dict):
            # If tool_output has an 'answer' field from Tavily, use it
            if "answer" in tool_output:
                web_answer = tool_output.get("answer", "")
                results = tool_output.get("results", [])
            else:
                web_answer = ""
                results = tool_output.get("results", [])
        else:
            web_answer = ""
            results = []
        
        # Format sources
        sources_text = []
        for i, result in enumerate(results[:3], 1):
            if isinstance(result, dict):
                title = result.get("title", "")
                content = result.get("content", "")
                if title and content:
                    sources_text.append(f"{i}. {title}\n{content[:200]}...")
        
        context = f"Web search summary: {web_answer}\n\nSources:\n" + "\n\n".join(sources_text)
        
        system_prompt = "You are a helpful AI assistant. Provide a clear, factual answer based on the web search results."
        user_prompt = f"{context}\n\nQuestion: {user_question}\n\nAnswer:"
    
    else:
        # Generic fallback
        system_prompt = "You are a helpful AI assistant. Answer the question based on the provided information."
        user_prompt = f"Information:\n{json.dumps(tool_output)}\n\nQuestion: {user_question}\n\nAnswer:"
    
    # Call LLM to generate human-readable answer
    answer = call_llm(system_prompt, user_prompt)
    return answer


def execute(tool_name: str, user_question: str, fallback_to_vector: bool = True, fallback_to_web: bool = True) -> Dict[str, Any]:
    """
    Execute the specified tool with the user's question and generate a human-readable answer.
    
    Implements automatic fallback chain:
    - graph_search -> vector_search -> web_search
    - vector_search -> web_search
    
    Args:
        tool_name: Name of the tool to execute 
                   ("direct_answer", "vector_search", "graph_search", "web_search")
        user_question: The user's input question
        fallback_to_vector: If True, fallback to vector_search when graph_search returns empty
        fallback_to_web: If True, fallback to web_search when internal tools return empty
        
    Returns:
        Dict containing:
            - answer: Human-readable answer generated by LLM
            - source: The tool that was used (e.g., "graph_search", "vector_search", "web_search")
        Includes 'fallback_used' and 'original_tool' keys when fallback occurs
        
    Raises:
        ValueError: If tool_name is invalid
        RuntimeError: If tool execution fails
    """
    
    valid_tools = ["direct_answer", "vector_search", "graph_search", "web_search"]
    
    if tool_name not in valid_tools:
        raise ValueError(f"Invalid tool '{tool_name}'. Must be one of {valid_tools}")
    
    try:
        if tool_name == "direct_answer":
            # No tool execution needed for direct answers - generate response directly
            system_prompt = "You are a helpful AI assistant. Respond naturally and conversationally."
            answer = call_llm(system_prompt, user_question)
            return {
                "answer": answer,
                "source": "direct_answer"
            }
        
        elif tool_name == "vector_search":
            # Execute vector search on Pinecone
            results = pinecone_tool.search(user_question)
            
            # Check if results are empty and web fallback is enabled
            if fallback_to_web and (not results or len(results) == 0):
                # Vector search returned nothing - fallback to web search
                print("Vector search returned no results. Falling back to web search...")
                web_results = web_search_tool.search(user_question)
                
                # Generate answer from web results
                answer = _generate_answer_from_tool_output(user_question, web_results, "web_search")
                
                return {
                    "answer": answer,
                    "source": "web_search",
                    "fallback_used": True,
                    "original_tool": "vector_search"
                }
            
            # Generate answer from vector search results
            answer = _generate_answer_from_tool_output(user_question, results, "vector_search")
            
            return {
                "answer": answer,
                "source": "vector_search"
            }
        
        elif tool_name == "graph_search":
            # Execute graph search on Neo4j
            results = neo4j_tool.query(user_question)
            
            # Check if results are empty and fallback is enabled
            if fallback_to_vector and (not results or len(results) == 0):
                # Graph search returned nothing - fallback to vector search
                print("Graph search returned no results. Falling back to vector search...")
                vector_results = pinecone_tool.search(user_question)
                
                # If vector also returns nothing, fallback to web
                if fallback_to_web and (not vector_results or len(vector_results) == 0):
                    print("Vector search also returned no results. Falling back to web search...")
                    web_results = web_search_tool.search(user_question)
                    
                    # Generate answer from web results
                    answer = _generate_answer_from_tool_output(user_question, web_results, "web_search")
                    
                    return {
                        "answer": answer,
                        "source": "web_search",
                        "fallback_used": True,
                        "original_tool": "graph_search",
                        "fallback_chain": "graph_search -> vector_search -> web_search"
                    }
                
                # Generate answer from vector search results
                answer = _generate_answer_from_tool_output(user_question, vector_results, "vector_search")
                
                return {
                    "answer": answer,
                    "source": "vector_search",
                    "fallback_used": True,
                    "original_tool": "graph_search"
                }
            
            # Generate answer from graph search results
            answer = _generate_answer_from_tool_output(user_question, results, "graph_search")
            
            return {
                "answer": answer,
                "source": "graph_search"
            }
        
        elif tool_name == "web_search":
            # Execute web search
            results = web_search_tool.search(user_question)
            
            # Generate answer from web search results
            answer = _generate_answer_from_tool_output(user_question, results, "web_search")
            
            return {
                "answer": answer,
                "source": "web_search"
            }
            
    except Exception as e:
        raise RuntimeError(f"Error executing tool '{tool_name}': {e}")


if __name__ == "__main__":
    # Test the executor with different tools
    print("Testing Executor:\n")
    
    test_cases = [
        ("direct_answer", "Hello!"),
        ("vector_search", "What is the vacation policy?"),
        ("graph_search", "Who reports to the CEO?"),
        ("web_search", "Latest AI news"),
    ]
    
    for tool, question in test_cases:
        print(f"Tool: {tool}")
        print(f"Question: {question}")
        try:
            result = execute(tool, question)
            print(f"Result: {result}\n")
        except Exception as e:
            print(f"Error: {e}\n")
